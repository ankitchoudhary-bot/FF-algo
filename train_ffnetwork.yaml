name: Train FFNetwork on MNIST
description: Trains a custom FFNetwork model using preprocessed MNIST dataset with forward-forward learning.

inputs:
  - {name: epochs, type: int, default: 60}
  - {name: x_train_norm, type: Dataset}
  - {name: y_train_int, type: Dataset}
  - {name: output_model_path, type: string, description: "Path to save the trained model"}

outputs:
  - {name: model_output, type: Dataset}

implementation:
  container:
    image: tensorflow/tensorflow:2.15.0
    command:
      - sh
      - -c
      - |
        pip install --quiet scikit-learn matplotlib keras || exit 1
        exec python3 -u -c "$0" "$@"
      - |
        import os
        import numpy as np
        import tensorflow as tf
        import keras
        from keras import ops
        from sklearn.metrics import accuracy_score
        import argparse
        from tensorflow.compiler.tf2xla.python import xla
        import shutil

        parser = argparse.ArgumentParser()
        parser.add_argument("--epochs", type=int, required=True)
        parser.add_argument("--x_train_norm", type=str, required=True)
        parser.add_argument("--y_train_int", type=str, required=True)
        parser.add_argument("--output_model_path", type=str, required=True)
        parser.add_argument("--model_output", type=str, required=True)
        args = parser.parse_args()

        os.environ["KERAS_BACKEND"] = "tensorflow"

        x_train = np.load(args.x_train_norm)
        y_train = np.load(args.y_train_int)

        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)

        class FFDense(keras.layers.Layer):
            def __init__(self, units, init_optimizer, loss_metric, num_epochs=50, **kwargs):
                super().__init__(**kwargs)
                self.dense = keras.layers.Dense(units)
                self.relu = keras.layers.ReLU()
                self.optimizer = init_optimizer()
                self.loss_metric = loss_metric
                self.threshold = 1.5
                self.num_epochs = num_epochs

            def call(self, x):
                x_norm = ops.norm(x, ord=2, axis=1, keepdims=True) + 1e-4
                x_dir = x / x_norm
                return self.relu(self.dense(x_dir))

            def forward_forward(self, x_pos, x_neg):
                for _ in range(self.num_epochs):
                    with tf.GradientTape() as tape:
                        g_pos = ops.mean(ops.power(self.call(x_pos), 2), 1)
                        g_neg = ops.mean(ops.power(self.call(x_neg), 2), 1)
                        loss = ops.log(1 + ops.exp(ops.concatenate([-g_pos + self.threshold, g_neg - self.threshold], 0)))
                        mean_loss = ops.cast(ops.mean(loss), "float32")
                        self.loss_metric.update_state([mean_loss])
                    gradients = tape.gradient(mean_loss, self.dense.trainable_weights)
                    self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))
                return (
                    ops.stop_gradient(self.call(x_pos)),
                    ops.stop_gradient(self.call(x_neg)),
                    self.loss_metric.result()
                )

        class FFNetwork(keras.Model):
            def __init__(self, dims, init_layer_optimizer=lambda: keras.optimizers.Adam(learning_rate=0.03), **kwargs):
                super().__init__(**kwargs)
                self.init_layer_optimizer = init_layer_optimizer
                self.loss_var = keras.Variable(0.0, trainable=False)
                self.loss_count = keras.Variable(0.0, trainable=False)
                self.layer_list = [keras.Input(shape=(dims[0],))]
                for d in range(len(dims) - 1):
                    self.layer_list.append(FFDense(dims[d+1], self.init_layer_optimizer, keras.metrics.Mean()))

            @tf.function
            def overlay_y_on_x(self, data):
                X_sample, y_sample = data
                max_sample = ops.amax(X_sample, axis=0, keepdims=True)
                X_zeros = ops.zeros([10], dtype="float64")
                X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])
                X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])
                return X_sample, y_sample

            @tf.function
            def train_step(self, data):
                x, y = data
                x = ops.reshape(x, [-1, ops.shape(x)[1] * ops.shape(x)[2]])
                x_pos, y = ops.vectorized_map(self.overlay_y_on_x, (x, y))
                random_y = tf.random.shuffle(y)
                x_neg, _ = tf.map_fn(self.overlay_y_on_x, (x, random_y), dtype=(tf.float64, tf.int64))
                h_pos, h_neg = x_pos, x_neg
                for layer in self.layers:
                    if isinstance(layer, FFDense):
                        h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)
                        self.loss_var.assign_add(loss)
                        self.loss_count.assign_add(1.0)
                mean_res = ops.divide(self.loss_var, self.loss_count)
                return {"FinalLoss": mean_res}

        model = FFNetwork(dims=[784, 500, 500])
        model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.03), loss="mse", jit_compile=False)

        model.fit(train_dataset, epochs=args.epochs)
        model.save(args.output_model_path)
        shutil.copy(args.output_model_path, args.model_output)
    args:
      - --epochs
      - {inputValue: epochs}
      - --x_train_norm
      - {inputPath: x_train_norm}
      - --y_train_int
      - {inputPath: y_train_int}
      - --output_model_path
      - /tmp/ffnetwork_model.keras
      - --model_output
      - {outputPath: model_output}
